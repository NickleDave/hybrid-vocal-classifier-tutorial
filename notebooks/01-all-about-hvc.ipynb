{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hybrid vocal classifier: voice-to-text for songbirds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is it?\n",
    "A python library that uses machine learning algorithms to automate segmenting and labeling birdsong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may already know if you are reading this, birdsong consists of repeated elements, often called syllables or notes. We often segment a sound file containing birdsong into syllables by finding threshold crossings of the amplitude (or some similar method). Then we apply labels to those segments to understand something about the song (how its acoustics change during learning, or compare across individuals, or whether song has some sort of syntax, etc.).\n",
    "![bengalse finch song](../static/bf_bird1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how does it work?\n",
    "The *hybrid-vocal-classifier* library (henceforth, `hvc`) uses **supervised learning** algorithms. \"Supervised\" means that these algorithms need some ground truth data that they use during training to build **classifiers** that make accurate predictions segmentation for new, unseen data. (Sometimes people say **models** -- for our purposes, that's synonymous with a classifier).\n",
    "\n",
    "In other words, you have to label a little bit of song by hand, so the algorithms can \"learn\" the correct segmentation and labels. Then `hvc` does the rest of the work for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how do I use it?\n",
    "That's what we're about to go over. But at a high level, there's three steps:\n",
    "1. extract features that the machine learning algorithms use during training\n",
    "> these can be any feature you derive from audio, either specific parameters like pitch and duration, or even a \"window\" of the spectrogram used like an image that is input to a computer-vision algorithm\n",
    "2. select models/classifiers based on their accuracy\n",
    "3. apply the models/classifiers you train (with the features you extracted) to new audio files (by extracting the same features from those files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why did you make this?\n",
    "Of course it's nice to have automate processes in the lab, but there are other goals for the library as well. One goal is to make it easy for you to compare all the different algorithms people have proposed, using the data you have in your lab, so you can see for yourself which one works best for your needs. Another goal is to help you figure out just how much data you have to label to get \"good enough\" accuracy for your analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
